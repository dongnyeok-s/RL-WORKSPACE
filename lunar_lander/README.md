# LunarLander â€” 4ëŒ€ RL ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹¤í—˜

Gymnasium ë‚´ì¥ LunarLander í™˜ê²½ì—ì„œ **PPO, DQN, SAC, REINFORCE** 4ê°œ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê³  ë¹„êµí•œë‹¤.

ì´ì‚°(Discrete)ê³¼ ì—°ì†(Continuous) í–‰ë™ ê³µê°„ì„ ëª¨ë‘ ë‹¤ë£¨ì–´ On-Policy vs Off-Policy, Value-based vs Policy-basedì˜ ì°¨ì´ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•œë‹¤.

---

## í™˜ê²½

```
           â•²     â•±
            â•²   â•±
             â•² â•±     â† ë‹¬ ì°©ë¥™ì„ 
            â”Œâ”€â”´â”€â”
          ğŸ”¥â”‚   â”‚ğŸ”¥   â† ì¢Œìš° ì—”ì§„
            â””â”€â”€â”€â”˜
               ğŸ”¥     â† ë©”ì¸ ì—”ì§„
    â”€ â”€ â”€ â”€ ğŸ â”€ â”€ â”€ â”€  â† ì°©ì§€ íŒ¨ë“œ
```

| í•­ëª© | Discrete | Continuous |
|------|----------|------------|
| ê´€ì¸¡ | 8ì°¨ì› (x, y, vx, vy, angle, angular_vel, left_leg, right_leg) | ë™ì¼ |
| í–‰ë™ | 4ê°œ (noop, left, main, right) | 2ì°¨ì› (main [-1,1], lateral [-1,1]) |
| ë³´ìƒ | ì°©ì§€ +100~140, ì¶”ë½ -100, ì—°ë£Œ ì†Œë¹„ | ë™ì¼ |
| í•´ê²° | í‰ê·  ë³´ìƒ >= 200 | ë™ì¼ |

---

## ì•Œê³ ë¦¬ì¦˜

| ì•Œê³ ë¦¬ì¦˜ | íƒ€ì… | í™˜ê²½ | í•µì‹¬ íŠ¹ì§• |
|---------|------|------|----------|
| **PPO** | On-Policy, Actor-Critic | Discrete + Continuous | Clip Loss, GAE, Multiple Epochs |
| **DQN** | Off-Policy, Value-based | Discrete | Double DQN, Target Net, Îµ-greedy |
| **SAC** | Off-Policy, Actor-Critic | Continuous | Twin Q, Entropy Max, ìë™ Î± |
| **REINFORCE** | On-Policy, Policy Gradient | Discrete | Baseline ë¹„êµìš© (Vanilla PG) |

---

## íŒŒì¼ êµ¬ì¡°

```
lunar_lander/
â”œâ”€â”€ README.md
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.py                 ê³µìœ  ì„¤ì •
â”‚   â”œâ”€â”€ ppo_discrete.py
â”‚   â”œâ”€â”€ ppo_continuous.py
â”‚   â”œâ”€â”€ reinforce.py
â”‚   â”œâ”€â”€ dqn.py
â”‚   â””â”€â”€ sac.py
â”‚
â”œâ”€â”€ networks/
â”‚   â”œâ”€â”€ actor_critic_discrete.py    Categorical ActorCritic (PPO ì´ì‚°)
â”‚   â”œâ”€â”€ actor_critic_continuous.py  Gaussian ActorCritic (PPO ì—°ì†)
â”‚   â”œâ”€â”€ policy_net.py               PolicyNet (REINFORCE)
â”‚   â”œâ”€â”€ q_network.py                QNetwork (DQN)
â”‚   â””â”€â”€ sac_networks.py             TwinQ + GaussianPolicy (SAC)
â”‚
â”œâ”€â”€ buffers/
â”‚   â”œâ”€â”€ rollout_buffer.py           On-Policy + GAE (PPOìš©)
â”‚   â””â”€â”€ replay_buffer.py            Off-Policy (DQN/SACìš©)
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ppo_agent.py                PPO (discrete/continuous ëª¨ë“œ)
â”‚   â”œâ”€â”€ reinforce_agent.py          REINFORCE
â”‚   â”œâ”€â”€ dqn_agent.py                Double DQN
â”‚   â””â”€â”€ sac_agent.py                SAC
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_ppo.py                PPO (--continuous í”Œë˜ê·¸)
â”‚   â”œâ”€â”€ train_reinforce.py
â”‚   â”œâ”€â”€ train_dqn.py
â”‚   â”œâ”€â”€ train_sac.py
â”‚   â””â”€â”€ run_all.py                  5ê°œ ë³€í˜• ìˆœì°¨ ì‹¤í–‰
â”‚
â”œâ”€â”€ evaluate.py                     í†µí•© í‰ê°€ + ë Œë”ë§
â”œâ”€â”€ compare.py                      ë¹„êµ ë¶„ì„ ì°¨íŠ¸ ìƒì„±
â””â”€â”€ results/
    â”œâ”€â”€ models/{algo}.pt
    â”œâ”€â”€ plots/{algo}.png
    â””â”€â”€ tensorboard/{algo}/
```

---

## ì‹¤í–‰ ë°©ë²•

```bash
# RL Workspace ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
cd "RL Workspace"

# ì˜ì¡´ì„± ì„¤ì¹˜
brew install swig
pip install gymnasium[box2d]

# â”€â”€ ê°œë³„ í•™ìŠµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m lunar_lander.train.train_ppo                # PPO Discrete
python -m lunar_lander.train.train_ppo --continuous   # PPO Continuous
python -m lunar_lander.train.train_reinforce          # REINFORCE
python -m lunar_lander.train.train_dqn                # DQN
python -m lunar_lander.train.train_sac                # SAC

# â”€â”€ ì „ì²´ ìˆœì°¨ í•™ìŠµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m lunar_lander.train.run_all

# â”€â”€ í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m lunar_lander.evaluate --algo ppo_discrete --render   # ë Œë”ë§
python -m lunar_lander.evaluate --algo sac --render
python -m lunar_lander.evaluate --all                          # ì „ì²´ ìˆ˜ì¹˜ í‰ê°€

# â”€â”€ ë¹„êµ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m lunar_lander.compare                         # ë¹„êµ ì°¨íŠ¸ ìƒì„±

# â”€â”€ TensorBoard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tensorboard --logdir lunar_lander/results/tensorboard
```

---

## ì½”ë“œ ì½ëŠ” ìˆœì„œ

```
1. configs/base.py          â€” ê³µìœ  ì„¤ì • í™•ì¸
2. networks/ ì¤‘ í•˜ë‚˜        â€” ì‹ ê²½ë§ êµ¬ì¡° ì´í•´
3. buffers/                 â€” On-Policy vs Off-Policy ë²„í¼ ì°¨ì´
4. agents/ ì¤‘ í•˜ë‚˜          â€” ì•Œê³ ë¦¬ì¦˜ ì—…ë°ì´íŠ¸ ë¡œì§
5. train/ ì¤‘ í•˜ë‚˜           â€” ì „ì²´ í•™ìŠµ ë£¨í”„
6. evaluate.py + compare.py â€” í‰ê°€ ë° ë¹„êµ
```

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | PPO | PPO-Cont | REINFORCE | DQN | SAC |
|---------|-----|----------|-----------|-----|-----|
| hidden_dim | 64 | 64 | 64 | 64 | 256 |
| lr | 3e-4 | 3e-4 | 1e-3 | 1e-3 | 3e-4 |
| gamma | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 |
| batch_size | 64 | 64 | - | 64 | 256 |
| buffer/rollout | 2048 | 2048 | - | 100K | 100K |
| ê³ ìœ  ì„¤ì • | clip=0.2, 10epochs | entropy=0 | - | Îµ: 1â†’0.05 | Î±: auto |

ì´ í•™ìŠµ ìŠ¤í…: 500K (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë™ì¼)

---

## í•µì‹¬ ì¸ì‚¬ì´íŠ¸

| ë¹„êµ ì¶• | ê²°ê³¼ |
|---------|------|
| **PPO vs REINFORCE** | PPOê°€ GAE + Clipìœ¼ë¡œ ì•ˆì •ì  ìˆ˜ë ´. REINFORCEëŠ” ê³ ë¶„ì‚° |
| **PPO vs DQN** | ì´ì‚° í™˜ê²½ì—ì„œ ë¹„ìŠ·í•œ ìµœì¢… ì„±ëŠ¥. DQNì´ ìƒ˜í”Œ íš¨ìœ¨ì  |
| **SAC vs PPO-Cont** | SACê°€ ì—°ì† í™˜ê²½ì—ì„œ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´ (Off-Policy íš¨ìœ¨ + ì—”íŠ¸ë¡œí”¼ íƒí—˜) |
| **On vs Off-Policy** | Off-Policy (DQN, SAC)ê°€ ë°ì´í„° ì¬ì‚¬ìš©ìœ¼ë¡œ ìƒ˜í”Œ íš¨ìœ¨ì  |
